import requests
import time
import json
from datetime import datetime, timedelta
from lxml import etree
import re

BASE_URL = "https://bakusai.com"
LIST_URL = "https://bakusai.com/thr_tl/acode=13/ctrid=1/ctgid=150/bid=2396/p={}/"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept-Language": "ja-JP,ja;q=0.9"
}

session = requests.Session()
session.headers.update(HEADERS)

# ========== è¯·æ±‚ ==========
def fetch(url):
    try:
        r = session.get(url, timeout=20)
        r.raise_for_status()
        return r.content  # è¿”å› bytesï¼Œé¿å… encoding declaration æŠ¥é”™
    except Exception as e:
        print("âš ï¸ è¯·æ±‚å¤±è´¥ï¼š", e)
        return None

# ========== æ¸…æ´—è¯„è®ºæ–‡æœ¬ ==========
def clean_comments_text(comments_list):
    all_text = []
    for c in comments_list:
        text = c["content"]
        text = re.sub(r'#\d+\s*[\d/:\s]*', '', text)  # å»æ‰ #æ•°å­—ã€æ—¥æœŸ
        text = re.sub(r'>>\d+', '', text)             # å»æ‰å¼•ç”¨
        text = text.replace('\r', '').replace('\n', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        if text:
            all_text.append(text)
    return '\n'.join(all_text)

# ========== è§£æåˆ—è¡¨é¡µæœ€åå›å¤æ—¶é—´ ==========
def parse_last_reply_time(text):
    text = text.strip()
    now = datetime.now()
    if "æ™‚é–“å‰" in text:
        h = int(re.search(r'(\d+)æ™‚é–“å‰', text).group(1))
        return now - timedelta(hours=h)
    elif "åˆ†å‰" in text:
        m = int(re.search(r'(\d+)åˆ†å‰', text).group(1))
        return now - timedelta(minutes=m)
    else:  # 12/11 21:12 å½¢å¼
        try:
            dt = datetime.strptime(text, "%m/%d %H:%M")
            return dt.replace(year=now.year)
        except:
            return None

# ========== è§£æåˆ—è¡¨é¡µ ==========
def parse_thread_list(page, current_year, current_month):
    print(f"ğŸ“„ æ­£åœ¨æŠ“åˆ—è¡¨é¡µ {page}")
    html = fetch(LIST_URL.format(page))
    if not html:
        return [], True

    tree = etree.HTML(html)
    threads = []
    stop = False

    for li in tree.xpath("//li[@data-tid]"):
        tid = li.get("data-tid")
        title = "".join(li.xpath(".//a[contains(@class,'thr_status_icon')]//text()")).strip()

        # åˆ—è¡¨é¡µçœŸå®è¯„è®ºæ•°
        comment_count_text = li.xpath(".//span[contains(@class,'comment_count_area')]/span[last()]/text()")
        try:
            comment_count = int(comment_count_text[0].strip())
        except:
            comment_count = 0

        # æœ€åä¸€æ¡å›å¤æ—¶é—´
        last_reply_text = "".join(li.xpath(".//span[@class='thr-posted-ago']//text()")).strip()
        last_reply_time = parse_last_reply_time(last_reply_text)
        if not last_reply_time:
            continue

        # åœæ­¢æ¡ä»¶ï¼šæœ€åå›å¤ä¸æ˜¯å½“æœˆ
        if last_reply_time.year != current_year or last_reply_time.month != current_month:
            stop = True
            continue

        if comment_count == 0:
            continue

        threads.append({
            "tid": tid,
            "title": title,
            "url": f"{BASE_URL}/thr_res/acode=13/ctrid=1/ctgid=150/bid=2396/tid={tid}/tp=1/",
            "comment_count": comment_count
        })

    print(f"    âœ” æœ¬é¡µè§£æåˆ° {len(threads)} ä¸ªæœ¬æœˆæœ‰å›å¤å¸–å­")
    return threads, stop

# ========== è§£æå¸–å­é¡µ ==========
def parse_thread_detail(thread):
    html = fetch(thread["url"])
    if not html:
        return None

    tree = etree.HTML(html)

    # å‘å¸–æ—¶é—´
    post_time_text = tree.xpath("//span[@class='posts' and @itemprop='datePublished']/text()")
    if post_time_text:
        post_time_text = post_time_text[0].strip()
        try:
            post_time = datetime.strptime(post_time_text, "%Y/%m/%d %H:%M").strftime("%Y-%m-%d %H:%M:%S")
        except:
            post_time = ""
    else:
        post_time = ""

    # å¸–å­æ­£æ–‡
    body = "".join(tree.xpath("//div[@id='threadBody']//text()")).strip()

    # è¯„è®º
    comments = []
    for idx, res in enumerate(tree.xpath("//div[contains(@class,'resbody')]")):
        if idx >= 100:  # æœ€å¤šæŠ“ 100 æ¡è¯„è®º
            break
        content = "".join(res.xpath(".//text()")).strip()
        if content:
            comments.append({"content": content})

    comments_text = clean_comments_text(comments)

    return {
        "url": thread["url"],
        "title": thread["title"],
        "comment_count": thread["comment_count"],
        "post_time": post_time,
        "body": body,
        "comments": comments_text
    }

# ========== ä¸»æµç¨‹ ==========
def crawl_current_month(max_pages=50):
    results = []
    now = datetime.now()
    current_year = now.year
    current_month = now.month

    for page in range(1, max_pages + 1):
        threads, stop = parse_thread_list(page, current_year, current_month)
        for t in threads:
            detail = parse_thread_detail(t)
            if not detail:
                continue
            results.append(detail)
            print(f"    âœ… æ”¶å½•å¸–å­ {t['tid']}ï¼ˆè¯„è®ºæ•°: {t['comment_count']}ï¼‰")
            time.sleep(1)

        if stop:
            print("ğŸ“Œ å·²åˆ°å½“æœˆæœ€åå›å¤å¸–å­ï¼Œåœæ­¢ç¿»é¡µ")
            break
        time.sleep(2)

    return results

# ========== å…¥å£ ==========
if __name__ == "__main__":
    data = crawl_current_month(max_pages=50)

    with open("bakusai_current_month.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å®Œæˆï¼šå…±æŠ“å– {len(data)} æ¡æœ¬æœˆå¸–å­")
