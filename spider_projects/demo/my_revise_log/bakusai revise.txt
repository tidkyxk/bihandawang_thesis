12.14 test1
import scrapy

class BakusaiSpider(scrapy.Spider):
    name = "bakusai"
    allowed_domains = ["bakusai.com"]
    start_urls = [
        "https://bakusai.com/areatop/acode=13/ctrid=1/"
    ]

    def parse(self, response):
        for post in response.css('.thread_list .thread_item'):
            yield {
                'title': post.css('.thread_title::text').get(),
                'link': post.css('a::attr(href)').get(),
                'author': post.css('.author_name::text').get(),
                'date': post.css('.post_date::text').get(),
            }

        next_page = response.css('a.next_page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
ç»“æžœï¼šnone

test 2
import scrapy


class BakusaiSpider(scrapy.Spider):
    name = "bakusai"
    allowed_domains = ["bakusai.com"]
    start_urls = [
        "https://bakusai.com/areatop/acode=13/ctrid=1/"
    ]

    def parse(self, response):
        for post in response.css("ul.thread li"):
            title = post.css("a::text").get()
            link = post.css("a::attr(href)").get()

            if title and link:
                yield {
                    "title": title.strip(),
                    "link": response.urljoin(link),
                }
ç»“æžœï¼š[

][

]

test3
import scrapy
import re


class BakusaiSpider(scrapy.Spider):
    name = "bakusai"
    allowed_domains = ["bakusai.com"]

    start_urls = [
        # ä¸œäº¬ â†’ æ”¿ç»/å›½é™…ç›¸å…³åŒºåŸŸï¼ˆä½ ä¹Ÿå¯ä»¥æ¢æˆåˆ«çš„åœ°åŒºï¼‰
        "https://bakusai.com/thr_tl/acode=13/ctrid=1/"
    ]

    # ä¸­å›½ç›¸å…³å…³é”®è¯ï¼ˆå¯è‡ªè¡Œæ‰©å±•ï¼‰
    CHINA_KEYWORDS = [
        "ä¸­å›½",
        "ä¸­å…±",
        "ä¸­å›½æ”¿åºœ",
        "åŒ—äº¬",
        "ç¿’è¿‘å¹³",
        "ç¿’ä¸»å¸­",
        "ç¿’",
        "CPC",
        "China",
    ]

    def normalize(self, text: str) -> str:
        """ç»Ÿä¸€æ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºŽå…³é”®è¯åŒ¹é…"""
        if not text:
            return ""
        text = text.lower()
        text = re.sub(r"\s+", "", text)
        return text

    # â‘  è¿›å…¥å„ä¸ªç‰ˆå—
    def parse(self, response):
        for a in response.css("a[href*='/thr_tl/']"):
            href = a.attrib.get("href")
            if href:
                yield response.follow(href, self.parse_thread_list)

    # â‘¡ åœ¨å¸–å­æ ‡é¢˜å±‚é¢ç­›é€‰â€œä¸­å›½ç›¸å…³â€
    def parse_thread_list(self, response):
        for a in response.css("a[href*='/thr_res/']"):
            title = a.xpath("string(.)").get()
            href = a.attrib.get("href")

            if not title or not href:
                continue

            title_norm = self.normalize(title)

            if any(self.normalize(k) in title_norm for k in self.CHINA_KEYWORDS):
                yield response.follow(
                    href,
                    self.parse_thread,
                    meta={"thread_title": title.strip()}
                )

        # ç¿»é¡µï¼ˆå¦‚æžœæœ‰ï¼‰
        next_page = response.css("a.next::attr(href)").get()
        if next_page:
            yield response.follow(next_page, self.parse_thread_list)

    # â‘¢ è§£æžå¸–å­è¯¦æƒ…é¡µ
    def parse_thread(self, response):
        content = response.xpath("//div[@id='write']//text()").getall()
        content = "\n".join(t.strip() for t in content if t.strip())

        yield {
            "thread_title": response.meta.get("thread_title"),
            "url": response.url,
            "content": content,
        }
test 4
import scrapy


class BakusaiSpider(scrapy.Spider):
    name = "bakusai"
    allowed_domains = ["bakusai.com"]

    start_urls = [
        "https://bakusai.com/areamain/acode=13/ctrid=1/"
    ]

    # â‘  ä¸­å›½ç‰ˆé¦–é¡µ â†’ å„å­ç‰ˆå—
    def parse(self, response):
        for a in response.css("a[href*='/thr_tl/']"):
            href = a.attrib.get("href")
            title = a.attrib.get("title", "").strip()

            if href:
                yield response.follow(
                    href,
                    self.parse_thread_list,
                    meta={"board_title": title}
                )

    # â‘¡ å­ç‰ˆå— â†’ å¸–å­åˆ—è¡¨
    def parse_thread_list(self, response):
        for a in response.css("a[href*='/thr_res/']"):
            href = a.attrib.get("href")
            title = a.attrib.get("title", "").strip()

            if href:
                yield response.follow(
                    href,
                    self.parse_thread,
                    meta={
                        "thread_title": title,
                        "board_title": response.meta.get("board_title"),
                    }
                )

        # ç¿»é¡µï¼ˆå¦‚å­˜åœ¨ï¼‰
        next_page = response.css("a.next::attr(href)").get()
        if next_page:
            yield response.follow(next_page, self.parse_thread_list)

    # â‘¢ å¸–å­è¯¦æƒ…é¡µ â†’ æ­£æ–‡
    def parse_thread(self, response):
        texts = response.xpath("//div[@id='write']//text()").getall()
        content = "\n".join(t.strip() for t in texts if t.strip())

        yield {
            "board": response.meta.get("board_title"),
            "title": response.meta.get("thread_title"),
            "url": response.url,
            "content": content,
        }
ç»“æžœï¼šæœ‰ï¼Œä½†æ˜¯è¢«å¸¦è·‘åï¼Œæœ‰å…¶ä»–å›½å®¶åœ°åŒºçš„å†…å®¹

test 5 åªé™å®šåœ¨ä¸­å›½åŒº
import scrapy


class BakusaiChinaSpider(scrapy.Spider):
    name = "bakusai_china"
    allowed_domains = ["bakusai.com"]

    # ä¸­å›½ç‰ˆé¦–é¡µ
    start_urls = [
        "https://bakusai.com/areamain/acode=13/ctrid=1/"
    ]

    # â‘  ä¸­å›½ç‰ˆé¦–é¡µ â†’ ä¸­å›½å­ç‰ˆå—ï¼ˆthr_tlï¼‰
    def parse(self, response):
        for a in response.css("a[href^='/thr_tl/']"):
            href = a.attrib.get("href")
            title = a.attrib.get("title", "").strip()

            # åªå…è®¸ä¸­å›½åŒº
            if not href or "/acode=13/" not in href:
                continue

            yield response.follow(
                href,
                callback=self.parse_thread_list,
                meta={
                    "board_title": title,
                }
            )

    # â‘¡ ä¸­å›½å­ç‰ˆå— â†’ ä¸­å›½å¸–å­åˆ—è¡¨ï¼ˆthr_resï¼‰
    def parse_thread_list(self, response):
        for a in response.css("a[href^='/thr_res/']"):
            href = a.attrib.get("href")
            title = a.attrib.get("title", "").strip()

            # å†æ¬¡ä¿é™©ï¼šåªçˆ¬ä¸­å›½åŒºå¸–å­
            if not href or "/acode=13/" not in href:
                continue

            yield response.follow(
                href,
                callback=self.parse_thread,
                meta={
                    "board_title": response.meta.get("board_title"),
                    "thread_title": title,
                }
            )

        # ç¿»é¡µï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        next_page = response.css("a.next::attr(href)").get()
        if next_page and "/acode=13/" in next_page:
            yield response.follow(next_page, self.parse_thread_list)

    # â‘¢ ä¸­å›½å¸–å­è¯¦æƒ…é¡µ â†’ æ­£æ–‡
    def parse_thread(self, response):
        # ä¿å®ˆæŠ“æ­£æ–‡ï¼ˆåŽé¢æˆ‘ä»¬ä¼šå†ç²¾ä¿®ï¼‰
        texts = response.xpath(
            "//div[contains(@class, 'res')]//text()"
        ).getall()

        content = "\n".join(
            t.strip() for t in texts if t.strip()
        )

        yield {
            "board": response.meta.get("board_title"),
            "title": response.meta.get("thread_title"),
            "url": response.url,
            "content": content,
        }

test 6 ä¸‰ã€ä¸ºä»€ä¹ˆä½ çŽ°åœ¨æŠ“åˆ°çš„ content ä¼šâ€œè¿™ä¹ˆè„â€

ä½ è´´å‡ºæ¥çš„ content é‡ŒåŒ…å«äº†ï¼š

âŒ ç«™å†…è§„åˆ™

âŒ JS ä»£ç 

âŒ è¡¨å•

âŒ ç¿»é¡µæŒ‰é’®

âŒ å¹¿å‘Šè„šæœ¬

âŒ æœ€æ–°ãƒ¬ã‚¹å£³å­

åŽŸå› åªæœ‰ä¸€ä¸ªï¼š

ä½ çŽ°åœ¨æ˜¯ response.xpath("string(.)") æˆ–ç›´æŽ¥ .get()
ðŸ‘‰ ç­‰äºŽæŠŠæ•´é¡µ HTML å½“æ­£æ–‡äº†
ä»¥ä¸‹ä¿®æ”¹åŽçš„
import scrapy
from urllib.parse import urljoin, urlparse, parse_qs


class BakusaiChinaSpider(scrapy.Spider):
    name = "bakusai_china"
    allowed_domains = ["bakusai.com"]

    start_urls = [
        # ä¸­å›½ç‰ˆ Â· æ”¿æ²»ç»æµŽ
        "https://bakusai.com/thr_tl/acode=13/ctrid=1/ctgid=137/"
    ]

    def parse(self, response):
        """
        è§£æžå¸–å­åˆ—è¡¨é¡µ
        """
        for href in response.css("a[href*='/thr_res/']::attr(href)").getall():
            url = urljoin(response.url, href)

            if self.is_china_thread(url):
                yield scrapy.Request(url, callback=self.parse_thread)

        # ç¿»é¡µï¼ˆbk=1,2,3...ï¼‰
        next_page = response.css("a.next::attr(href)").get()
        if next_page:
            yield response.follow(next_page, self.parse)

    def parse_thread(self, response):
        """
        è§£æžå¸–å­é¡µï¼ˆåªè¦æ­£æ–‡ï¼‰
        """
        title = response.css("h1::text").get()

        # æ¯ä¸€æ¡å›žå¤çš„æ­£æ–‡
        posts = response.css("div.res_body")

        contents = []
        for p in posts:
            text = p.xpath("string(.)").get()
            if text:
                contents.append(text.strip())

        yield {
            "title": title,
            "url": response.url,
            "content": "\n\n".join(contents)
        }

    def is_china_thread(self, url: str) -> bool:
        """
        ä¸¥æ ¼é™å®šï¼šä¸­å›½åŒºå¸–å­
        """
        qs = parse_qs(urlparse(url).query)

        return (
            qs.get("acode", [""])[0] == "13"
            and qs.get("ctrid", [""])[0] == "1"
        )

ç»“æžœï¼šå‡ºé—®é¢˜ï¼Œå•¥ä¹Ÿæ²¡æœ‰ã€ä¸ºä»€ä¹ˆåœ¨è¿™ä¸ªé¡µé¢ä¸Šã€ŒæŠ“ä¸åˆ°å¸–å­é“¾æŽ¥ã€ï¼Ÿä½ è¯·æ±‚çš„æ˜¯ï¼šhttps://bakusai.com/thr_tl/acode=13/ctrid=1/ctgid=137/
âš ï¸ è¿™ä¸ªé¡µé¢çš„çœŸå®žæƒ…å†µæ˜¯ï¼š
å®ƒä¸æ˜¯ä¸€ä¸ªâ€œç›´æŽ¥åˆ—å‡ºå¸–å­é“¾æŽ¥çš„æ™®é€š HTML é¡µé¢â€è€Œæ˜¯ï¼šä½¿ç”¨ frameset / iframeæˆ–è¿”å›žçš„æ˜¯ å¯¼èˆªå£³ + JSå¸–å­åˆ—è¡¨åœ¨ å¦ä¸€ä¸ª URL é‡ŒðŸ‘‰ Scrapy æ‹¿åˆ°çš„æ˜¯â€œå¤–å£³é¡µâ€ï¼Œä¸æ˜¯â€œå¸–å­åˆ—è¡¨é¡µâ€

test 7
import scrapy


class BakusaiChinaSpider(scrapy.Spider):
    name = "bakusai_china"
    allowed_domains = ["bakusai.com"]

    start_urls = [
        "https://bakusai.com/thr_tl/acode=13/ctrid=1/ctgid=137/"
    ]

    custom_settings = {
        "DOWNLOAD_DELAY": 1,
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Referer": "https://bakusai.com/",
        },
    }

    def parse(self, response):
        # éªŒè¯æ˜¯å¦è¿˜è¢« error æ‹¦
        if "ã‚¨ãƒ©ãƒ¼" in response.text:
            self.logger.error("âŒ è¢« bakusai æ‹¦æˆªï¼Œè¿”å›ž error é¡µé¢")
            return

        # æŠ“å¸–å­é“¾æŽ¥
        threads = response.css("a[href*='/thr_res/']")

        self.logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ä¸ªå¸–å­")

        for a in threads:
            yield {
                "title": a.xpath("string(.)").get(default="").strip(),
                "url": response.urljoin(a.attrib.get("href")),
            }
ç»“æžœï¼šå•¥ä¹Ÿæ²¡æœ‰ï¼Œè¢«åè¦æ›²å¥‡

test8
import scrapy


class BakusaiChinaSpider(scrapy.Spider):
    name = "bakusai_china"
    allowed_domains = ["bakusai.com"]

    custom_settings = {
        "DOWNLOAD_DELAY": 1,
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "FEED_EXPORT_ENCODING": "utf-8",
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        },
    }

    def start_requests(self):
        # ç¬¬ä¸€æ­¥ï¼šè®¿é—®é¦–é¡µï¼Œæ‹¿ Cookie
        yield scrapy.Request(
            url="https://bakusai.com/",
            callback=self.after_home,
        )

    def after_home(self, response):
        # ç¬¬äºŒæ­¥ï¼šå¸¦ç€ Cookie è®¿é—®ä¸­å›½åŒº
        china_url = "https://bakusai.com/thr_tl/acode=13/ctrid=1/ctgid=137/"

        yield response.follow(
            china_url,
            callback=self.parse_thread_list,
        )

    def parse_thread_list(self, response):
        if "ã‚¨ãƒ©ãƒ¼" in response.text:
            self.logger.error("âŒ ä»ç„¶æ˜¯ error é¡µé¢ï¼ˆCookie å¤±è´¥ï¼‰")
            return

        threads = response.css("a[href*='/thr_res/']")

        self.logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ä¸ªå¸–å­")

        for a in threads:
            yield {
                "title": a.xpath("string(.)").get(default="").strip(),
                "url": response.urljoin(a.attrib.get("href")),
            }
test 9 æ•°æ®æ¸…æ´—
import scrapy


class BakusaiNewsSpider(scrapy.Spider):
    name = "bakusai_news"
    allowed_domains = ["bakusai.com"]

    # ðŸ‘‰ è¿™é‡Œæ¢æˆä½ è‡ªå·±çš„ thr_res é¡µé¢
    start_urls = [
        "https://bakusai.com/thr_res/acode=3/ctgid=137/bid=1098/tid=13034308/"
    ]

    custom_settings = {
        "DOWNLOAD_DELAY": 1,
        "COOKIES_ENABLED": True,
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0 Safari/537.36"
            ),
            "Accept-Language": "ja-JP,ja;q=0.9",
        },
    }

    def parse(self, response):
        # ========= 1. æ–°é—»æ ‡é¢˜ =========
        title = response.css(
            "strong[itemprop='headline']::text"
        ).get(default="").strip()

        # ========= 2. æ–°é—»æ­£æ–‡ =========
        article_text = "\n".join(
            response.css("#threadBody ::text").getall()
        ).strip()

        # ========= 3. è¯„è®º =========
        comments = response.css("div.res_list_article")

        for c in comments:
            # æ¥¼å±‚ / è¯„è®ºID
            comment_id = c.attrib.get("id", "")

            # è¯„è®ºæ—¶é—´
            comment_time = c.css(
                "span[itemprop='commentTime']::text"
            ).get(default="").strip()

            # è¯„è®ºæ­£æ–‡
            comment_text = "\n".join(
                c.css("div.resbody ::text").getall()
            ).strip()

            # ç”¨æˆ·åï¼ˆä¸€èˆ¬æ˜¯åŒ¿åï¼‰
            username = c.css("dd.name::text").get(default="").strip()

            yield {
                "url": response.url,
                "title": title,
                "article_text": article_text,
                "comment_id": comment_id,
                "comment_time": comment_time,
                "username": username,
                "comment_text": comment_text,
                "language": "ja",
                "source": "bakusai",
            }

test 10æŒ‰æˆ‘å–œæ¬¢çš„æ–¹å¼æ¸…æ´—
import scrapy
from w3lib.html import remove_tags


class BakusaiChinaSpider(scrapy.Spider):
    name = "bakusai_china"
    allowed_domains = ["bakusai.com"]

    start_urls = [
        "https://bakusai.com/thr_tl/acode=13/ctrid=1/ctgid=150/bid=2396/"
    ]

    custom_settings = {
        "DOWNLOAD_DELAY": 1,
        "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    }

    def parse(self, response):
        # å¸–å­é“¾æŽ¥
        thread_links = response.css("a[href*='/thr_res/']::attr(href)").getall()

        for link in thread_links:
            yield response.follow(link, callback=self.parse_thread)

    def parse_thread(self, response):
        # ---------- 1. æ ‡é¢˜ï¼ˆä¸¤çº§å…œåº•ï¼‰ ----------
        title = response.css("strong[itemprop='headline']::text").get()
        if not title:
            title = response.css("title::text").get()

        if title:
            title = title.strip()

        # ---------- 2. æ–°é—»æ­£æ–‡ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰ ----------
        article_html = response.css("#threadBody").get()
        article_text = ""

        if article_html:
            article_text = remove_tags(article_html)
            article_text = article_text.replace("\n", " ").strip()

        # ---------- 3. è¯„è®ºï¼ˆè¿™æ˜¯é‡ç‚¹ï¼‰ ----------
        comments = response.css(
            "div.article.res_list_article div.resbody::text"
        ).getall()

        comments = [
            c.strip()
            for c in comments
            if c and c.strip()
        ]

        # ---------- 4. å…³é”®è¿‡æ»¤é€»è¾‘ ----------
        # è¿žè¯„è®ºéƒ½æ²¡æœ‰çš„ï¼Œç›´æŽ¥ä¸è¦
        if not comments:
            return

        yield {
            "url": response.url,
            "title": title,
            "article_text": article_text,
            "comments": comments,
            "language": "ja",
            "source": "bakusai",
        }
æœ€ç»ˆ
import scrapy
from w3lib.html import remove_tags
import re


class BakusaiChinaNewsSpider(scrapy.Spider):
    name = "bakusai_china_news"
    allowed_domains = ["bakusai.com"]

    # æ–°é—»åˆ—è¡¨é¡µ
    start_urls = [
        "https://bakusai.com/areamain/acode=13/ctrid=1/"
    ]

    custom_settings = {
        "DOWNLOAD_DELAY": 1,
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "FEED_EXPORT_ENCODING": "utf-8",
        "USER_AGENT": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    }

    def parse(self, response):
        """
        åˆ—è¡¨é¡µè§£æžï¼š
        - æŠ“æ–°é—»é“¾æŽ¥ï¼ˆctgid=137ï¼‰
        """
        self.logger.info(f"å½“å‰æŠ“å–é¡µé¢: {response.url}")

        news_links = response.css(
            "a[href^='/thr_res/'][href*='ctgid=137']::attr(href)"
        ).getall()

        self.logger.info(f"âœ… å‘çŽ°æ–°é—»é“¾æŽ¥ {len(news_links)} æ¡")

        for href in news_links:
            url = response.urljoin(href)
            yield scrapy.Request(
                url=url,
                callback=self.parse_detail
            )

    def parse_detail(self, response):
        """
        æ–°é—»è¯¦æƒ…é¡µè§£æžï¼š
        - æå–æ ‡é¢˜
        - æå–æ­£æ–‡ï¼ˆæ¸…ç†åˆ¶è¡¨ç¬¦ã€æ¢è¡Œå’Œè¿žç»­ç©ºæ ¼ï¼‰
        - æå–è¯„è®º
        """
        # ---------- 1ï¸âƒ£ æ ‡é¢˜ ----------
        title = response.css("strong[itemprop='headline']::text").get()
        if not title:
            title = response.css("h1::text").get()
        if title:
            title = title.strip()
        else:
            return  # æ²¡æ ‡é¢˜å°±ä¸è¦äº†

        # ---------- 2ï¸âƒ£ æ­£æ–‡ ----------
        article_html = response.css("div#threadBody[itemprop='articlebody']").get()
        article_text = ""
        if article_html:
            article_text = remove_tags(article_html)
            # åŽ»æŽ‰åˆ¶è¡¨ç¬¦ã€æ¢è¡Œå’Œè¿žç»­ç©ºæ ¼
            article_text = re.sub(r"[\t\r\n]+", " ", article_text)
            article_text = re.sub(r"\s{2,}", " ", article_text)
            article_text = article_text.strip()

        # ---------- 3ï¸âƒ£ è¯„è®º ----------
        raw_comments = response.css("div.resbody[itemprop='commentText'] ::text").getall()
        comments = []
        for c in raw_comments:
            c = c.strip()
            if not c:
                continue
            if c.startswith(">>") and c[2:].isdigit():
                continue
            if len(c) < 3:
                continue
            comments.append(c)

        # ---------- 4ï¸âƒ£ è¾“å‡º ----------
        yield {
            "title": title,
            "article_text": article_text,
            "comments": comments
        }

